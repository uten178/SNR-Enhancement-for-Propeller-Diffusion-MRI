{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io09d1iduq-M",
        "outputId": "c0b5484e-6ca5-479d-fcfe-e666dc74fb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViWrAfQQur52"
      },
      "outputs": [],
      "source": [
        "data_path = 'Propeller_FSE_DWI'\n",
        "sys.path.insert(0,data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-fKAqphu48K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Add, Subtract, Lambda, LayerNormalization,\\\n",
        "    LeakyReLU, Conv2D, MaxPooling2D, UpSampling2D, \\\n",
        "    Conv2DTranspose, Dropout, Concatenate,concatenate, SeparableConv2D, PReLU\n",
        "from tensorflow.keras.models import Model\n",
        "# Deep learning packages\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time, pdb, os\n",
        "import scipy.io as sio\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u4-3AL6P7ct"
      },
      "outputs": [],
      "source": [
        "## RU-Net\n",
        "def batchnorm_relu(inputs):\n",
        "     \"\"\" Batch Normalization & ReLU \"\"\"\n",
        "     x = BatchNormalization()(inputs)\n",
        "     x = Activation(\"relu\")(x)\n",
        "     x = Dropout(0.05)(x)\n",
        "     return x\n",
        "\n",
        "def residual_block(inputs, num_filters, strides=1):\n",
        "     \"\"\" Convolutional Layers \"\"\"\n",
        "     x = batchnorm_relu(inputs)\n",
        "     x = Conv2D(num_filters, 3, padding=\"same\", strides=strides)(x)\n",
        "     x = batchnorm_relu(x)\n",
        "     x = Conv2D(num_filters, 3, padding=\"same\", strides=1)(x)\n",
        "     \"\"\" Shortcut Connection (Identity Mapping) \"\"\"\n",
        "     s = Conv2D(num_filters, 1, padding=\"same\", strides=strides)(inputs)\n",
        "     \"\"\" Addition \"\"\"\n",
        "     x = x + s\n",
        "     return x\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "     \"\"\" Decoder Block \"\"\"\n",
        "     x = UpSampling2D((2, 2))(inputs) \n",
        "     x = Concatenate()([x, skip_features]) \n",
        "     x = residual_block(x, num_filters, strides=1) \n",
        "     return x\n",
        "\n",
        "def build_resunet(input_shape,sc):\n",
        "     \"\"\" RESUNET Architecture \"\"\"\n",
        "    \n",
        "     inputs = Input(input_shape)\n",
        "\n",
        "     \"\"\" Endoder 1 \"\"\" \n",
        "     x = Conv2D(64*sc, 3, padding=\"same\", strides=1)(inputs) \n",
        "     x = batchnorm_relu(x) \n",
        "     x = Conv2D(64*sc, 3, padding=\"same\", strides=1)(x) \n",
        "     s = Conv2D(64*sc, 1, padding=\"same\")(inputs) \n",
        "     s1 = x + s\n",
        "\n",
        "     \"\"\" Encoder 2, 3 \"\"\"\n",
        "     s2 = residual_block(s1, 128*sc, strides=2)\n",
        "     s3 = residual_block(s2, 256*sc, strides=2)\n",
        "\n",
        "     \"\"\" Bridge \"\"\"\n",
        "     b = residual_block(s3, 512*sc, strides=2)\n",
        "\n",
        "     \"\"\" Decoder 1, 2, 3 \"\"\" \n",
        "     x = decoder_block(b, s3, 256*sc) \n",
        "     x = decoder_block(x, s2, 128*sc) \n",
        "     x = decoder_block(x, s1, 64*sc)\n",
        "\n",
        "     \"\"\" Classifier \"\"\"\n",
        "     outputs = Conv2D(1, 1, padding=\"same\", activation=None)(x)\n",
        "\n",
        "     \"\"\" Model \"\"\"\n",
        "     model = Model(inputs, outputs, name=\"RESUNET\")\n",
        "\n",
        "     return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSkvoYUjvCLY"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, filename_list,num_rows, num_cols,batch_size,\n",
        "                 shuffle=True):\n",
        "\n",
        "        self.filename_list = filename_list\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_samples = len(self.filename_list)\n",
        "        self.num_rows = num_rows\n",
        "        self.num_cols = num_cols\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            #Shuffle the filename list in-place\n",
        "            np.random.shuffle(self.filename_list)\n",
        "\n",
        "    def __get_data(self, filenames):\n",
        "\n",
        "        noise_img = np.empty((self.batch_size,self.num_rows,self.num_cols,1))\n",
        "        label_img = np.empty((self.batch_size,self.num_rows,self.num_cols,1))\n",
        "        \n",
        "        for idx, curr_filename in enumerate(filenames):\n",
        "            noise_img[idx,], label_img[idx,] = self.prepare_single_input_output_pair(curr_filename)\n",
        "        return noise_img, label_img\n",
        "\n",
        "    # Return the index'th batch\n",
        "    def __getitem__(self, index):\n",
        "        curr_filenames = self.filename_list[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X, y = self.__get_data(curr_filenames)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples // self.batch_size\n",
        "\n",
        "    def prepare_single_input_output_pair(self,filename_one_sample):\n",
        "        temp = sio.loadmat(filename_one_sample)\n",
        "        noise_img = temp['ImL']\n",
        "        noise_img = noise_img/np.max(noise_img)\n",
        "        noise_img = tf.expand_dims(noise_img,axis=-1)\n",
        "        label_img = temp['Label']\n",
        "        label_img = label_img/np.max(label_img)\n",
        "        label_img = tf.expand_dims(label_img,axis=-1)\n",
        "     \n",
        "        return noise_img, label_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt6QxD8nv75t"
      },
      "outputs": [],
      "source": [
        "num_rows = 128\n",
        "num_cols = 128\n",
        "num_batchs = 8\n",
        "\n",
        "input_paths = glob.glob(os.path.join(data_path, \"Train/*.mat\"))\n",
        "decode = tf.image.decode_png\n",
        "\n",
        "if len(input_paths) == 0:\n",
        "    raise Exception(\"input_dir contains no image files\")\n",
        "\n",
        "def get_name(path):\n",
        "    name, _ = os.path.splitext(os.path.basename(path))\n",
        "    return name\n",
        "\n",
        "    # if the image names are numbers, sort by the value rather than asciibetically\n",
        "    # having sorted inputs means that the outputs are sorted in test mode\n",
        "if all(get_name(path).isdigit() for path in input_paths):\n",
        "    input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\n",
        "else:\n",
        "    input_paths = sorted(input_paths)\n",
        "\n",
        "training_gen = DataGenerator(input_paths,num_rows, num_cols,num_batchs,shuffle=True)\n",
        "#######################################################################################################################\n",
        "input_paths = glob.glob(os.path.join(data_path, \"Validate/*.mat\"))\n",
        "decode = tf.image.decode_png\n",
        "\n",
        "if len(input_paths) == 0:\n",
        "    raise Exception(\"input_dir contains no image files\")\n",
        "\n",
        "def get_name(path):\n",
        "    name, _ = os.path.splitext(os.path.basename(path))\n",
        "    return name\n",
        "\n",
        "    # if the image names are numbers, sort by the value rather than asciibetically\n",
        "    # having sorted inputs means that the outputs are sorted in test mode\n",
        "if all(get_name(path).isdigit() for path in input_paths):\n",
        "    input_paths = sorted(input_paths, key=lambda path: int(get_name(path)))\n",
        "else:\n",
        "    input_paths = sorted(input_paths)\n",
        "\n",
        "val_gen = DataGenerator(input_paths,num_rows, num_cols, num_batchs, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The model for 32M trainable parameters"
      ],
      "metadata": {
        "id": "UHvnK5qDVTXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8WZWciFv_Hy"
      },
      "outputs": [],
      "source": [
        "# \"Important\" parameters\n",
        "num_epochs = 200\n",
        "loss = 'nrmse'\n",
        "learning_rate_base = 1e-4\n",
        "batch_size = num_batchs\n",
        "\n",
        "\n",
        "tensorboard_filepath = os.path.join(data_path,'results')\n",
        "model_checkpoint_filepath = os.path.join(data_path,'trained_weights', 'Res_UNet.h5')\n",
        "pretrained_inet_path = os.path.join(data_path,'trained_weights', 'Res_UNet.h5')\n",
        "\n",
        "model = build_resunet(input_shape=(num_rows,num_cols,1),sc=2)\n",
        "\n",
        "if pretrained_inet_path is not None:\n",
        "   model.load_weights(pretrained_inet_path)\n",
        "\n",
        "# Compile the model\n",
        "adam_opt = Adam(learning_rate=learning_rate_base, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
        "tbCallBack = TensorBoard(log_dir=tensorboard_filepath, histogram_freq=0, write_graph=False, write_images=False)\n",
        "checkpointerCallBack = ModelCheckpoint(filepath=model_checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "if loss == 'nrmse':\n",
        "    # Objective function\n",
        "    def my_objective_function(y_true, y_pred):\n",
        "        return 100 * K.sqrt(K.sum(K.square(y_pred - y_true))) / K.sqrt(K.sum(K.square(y_true)))\n",
        "\n",
        "    model.compile(loss=my_objective_function, optimizer=adam_opt)\n",
        "else:\n",
        "    model.compile(loss=loss, optimizer=adam_opt)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrKRa8bnYb6o"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b0vcLpLZ8Xq"
      },
      "outputs": [],
      "source": [
        "hist = model.fit(x=training_gen, \n",
        "                     epochs=num_epochs, \n",
        "                     verbose=2, \n",
        "                     callbacks=[tbCallBack,checkpointerCallBack], \n",
        "                     validation_data=val_gen, \n",
        "                     shuffle=True, \n",
        "                     initial_epoch=0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}